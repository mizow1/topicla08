import os
import requests
from bs4 import BeautifulSoup
from anthropic import Anthropic
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
import json
import hashlib
from datetime import datetime

load_dotenv()

class SEOAnalyzer:
    def __init__(self):
        self.client = Anthropic(api_key=os.getenv('CLAUDE_API_KEY'))
        self.data_dir = 'seo_data'
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
        
    def fetch_url_content(self, url):
        """URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦HTMLã‚’è§£æ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            raise Exception(f"URLå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_page_info(self, html_content, url):
        """HTMLã‹ã‚‰SEOã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        page_info = {
            'url': url,
            'title': soup.title.string.strip() if soup.title else '',
            'meta_description': '',
            'h1_tags': [],
            'h2_tags': [],
            'h3_tags': [],
            'meta_keywords': '',
            'images_without_alt': 0,
            'total_images': 0,
            'internal_links': 0,
            'external_links': 0,
            'canonical_url': '',
            'meta_robots': '',
            'og_tags': {},
            'twitter_tags': {},
            'json_ld': [],
            'content_length': 0
        }
        
        # ãƒ¡ã‚¿æƒ…å ±ã®å–å¾—
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            page_info['meta_description'] = meta_desc.get('content', '')
            
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            page_info['meta_keywords'] = meta_keywords.get('content', '')
            
        meta_robots = soup.find('meta', attrs={'name': 'robots'})
        if meta_robots:
            page_info['meta_robots'] = meta_robots.get('content', '')
            
        # ã‚«ãƒãƒ‹ã‚«ãƒ«URL
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if canonical:
            page_info['canonical_url'] = canonical.get('href', '')
        
        # è¦‹å‡ºã—ã‚¿ã‚°ã®å–å¾—
        page_info['h1_tags'] = [h1.get_text().strip() for h1 in soup.find_all('h1')]
        page_info['h2_tags'] = [h2.get_text().strip() for h2 in soup.find_all('h2')]
        page_info['h3_tags'] = [h3.get_text().strip() for h3 in soup.find_all('h3')]
        
        # ç”»åƒã®åˆ†æ
        images = soup.find_all('img')
        page_info['total_images'] = len(images)
        page_info['images_without_alt'] = sum(1 for img in images if not img.get('alt'))
        
        # ãƒªãƒ³ã‚¯ã®åˆ†æ
        links = soup.find_all('a', href=True)
        domain = urlparse(url).netloc
        
        for link in links:
            href = link.get('href')
            if href.startswith('http'):
                if domain in href:
                    page_info['internal_links'] += 1
                else:
                    page_info['external_links'] += 1
            elif href.startswith('/'):
                page_info['internal_links'] += 1
        
        # OGã‚¿ã‚°
        og_tags = soup.find_all('meta', property=lambda x: x and x.startswith('og:'))
        for tag in og_tags:
            prop = tag.get('property')
            content = tag.get('content')
            if prop and content:
                page_info['og_tags'][prop] = content
                
        # Twitterã‚¿ã‚°
        twitter_tags = soup.find_all('meta', attrs={'name': lambda x: x and x.startswith('twitter:')})
        for tag in twitter_tags:
            name = tag.get('name')
            content = tag.get('content')
            if name and content:
                page_info['twitter_tags'][name] = content
        
        # JSON-LDæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                page_info['json_ld'].append(json.loads(script.string))
            except:
                pass
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·
        content_text = soup.get_text()
        page_info['content_length'] = len(content_text.strip())
        
        return page_info
    
    def analyze_with_claude(self, page_info):
        """Claude APIã‚’ä½¿ç”¨ã—ã¦SEOåˆ†æã¨æ”¹å–„æ¡ˆã‚’ç”Ÿæˆ"""
        
        analysis_prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®SEOæƒ…å ±ã‚’åˆ†æã—ã€SEOã®è¦³ç‚¹ã‹ã‚‰æ”¹å–„æ¡ˆã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

=== ãƒšãƒ¼ã‚¸æƒ…å ± ===
URL: {page_info['url']}
ã‚¿ã‚¤ãƒˆãƒ«: {page_info['title']}
ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³: {page_info['meta_description']}
H1ã‚¿ã‚°: {page_info['h1_tags']}
H2ã‚¿ã‚°: {page_info['h2_tags'][:5]}  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
H3ã‚¿ã‚°: {page_info['h3_tags'][:5]}  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
ç”»åƒæ•°: {page_info['total_images']}ï¼ˆaltå±æ€§ãªã—: {page_info['images_without_alt']}ï¼‰
å†…éƒ¨ãƒªãƒ³ã‚¯æ•°: {page_info['internal_links']}
å¤–éƒ¨ãƒªãƒ³ã‚¯æ•°: {page_info['external_links']}
ã‚«ãƒãƒ‹ã‚«ãƒ«URL: {page_info['canonical_url']}
ãƒ¡ã‚¿ãƒ­ãƒœãƒƒãƒ„: {page_info['meta_robots']}
OGã‚¿ã‚°: {page_info['og_tags']}
Twitterã‚«ãƒ¼ãƒ‰: {page_info['twitter_tags']}
æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿: {'ã‚ã‚Š' if page_info['json_ld'] else 'ãªã—'}
ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ–‡å­—æ•°: {page_info['content_length']}

=== åˆ†æè¦ä»¶ ===
1. ç¾çŠ¶ã®è‰¯ã„ç‚¹ã¨å•é¡Œç‚¹ã‚’æ•´ç†
2. SEOçš„è¦³ç‚¹ã§ã®å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’å„ªå…ˆåº¦ä»˜ãã§æç¤º
3. å„æ”¹å–„æ¡ˆã«ã¤ã„ã¦å®Ÿè£…æ–¹æ³•ã‚‚å«ã‚ã¦è©³ç´°ã«èª¬æ˜
4. ãƒ¢ãƒã‚¤ãƒ«ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€Core Web Vitalsã‚‚è€ƒæ…®
5. æ—¥æœ¬èªSEOã®ç‰¹æ€§ã‚‚è¸ã¾ãˆãŸææ¡ˆ

å›ç­”ã¯æ§‹é€ åŒ–ã•ã‚ŒãŸå½¢å¼ã§ã€å®Ÿè¡Œå¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã¨ã—ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚
"""

        try:
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                temperature=0.3,
                messages=[
                    {
                        "role": "user", 
                        "content": analysis_prompt
                    }
                ]
            )
            
            return response.content[0].text
            
        except Exception as e:
            raise Exception(f"Claude APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_url_hash(self, url):
        """URLã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()[:16]
    
    def get_saved_data_path(self, url):
        """ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
        url_hash = self.get_url_hash(url)
        return os.path.join(self.data_dir, f'seo_{url_hash}.json')
    
    def save_analysis_data(self, url, analysis_data):
        """åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        file_path = self.get_saved_data_path(url)
        data_to_save = {
            'url': url,
            'analysis_date': datetime.now().isoformat(),
            'page_info': analysis_data['page_info'],
            'seo_analysis': analysis_data['seo_analysis']
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        
        return file_path
    
    def load_saved_data(self, url):
        """ä¿å­˜ã•ã‚ŒãŸåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        file_path = self.get_saved_data_path(url)
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    def display_saved_analysis(self, saved_data):
        """ä¿å­˜ã•ã‚ŒãŸåˆ†æçµæœã‚’è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ“ ä¿å­˜æ¸ˆã¿åˆ†æçµæœã®è¡¨ç¤º")
        print("="*80)
        print(f"URL: {saved_data['url']}")
        print(f"åˆ†ææ—¥æ™‚: {saved_data['analysis_date']}")
        print("-" * 80)
        print(saved_data['seo_analysis'])
        print("="*80)
        return saved_data
    
    def analyze_url(self, url, force_reanalyze=False):
        """URLã‚’åˆ†æã—ã¦SEOæ”¹å–„æ¡ˆã‚’ç”Ÿæˆ"""
        if not force_reanalyze:
            saved_data = self.load_saved_data(url)
            if saved_data:
                print(f"ğŸ’¾ ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹: {saved_data['analysis_date']}")
                self.display_saved_analysis(saved_data)
                
                while True:
                    choice = input("\né¸æŠã—ã¦ãã ã•ã„: [S]ä¿å­˜ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ / [R]å†åˆ†æã™ã‚‹ / [Q]çµ‚äº†: ").upper()
                    if choice == 'S':
                        return {
                            'page_info': saved_data['page_info'],
                            'seo_analysis': saved_data['seo_analysis']
                        }
                    elif choice == 'R':
                        print("ğŸ”„ å†åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
                        break
                    elif choice == 'Q':
                        print("åˆ†æã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                        return None
                    else:
                        print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚S, R, Qã®ã„ãšã‚Œã‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        print(f"ğŸŒ URLå–å¾—ä¸­: {url}")
        html_content = self.fetch_url_content(url)
        
        print("ğŸ“Š ãƒšãƒ¼ã‚¸æƒ…å ±æŠ½å‡ºä¸­...")
        page_info = self.extract_page_info(html_content, url)
        
        print("ğŸ¤– Claude APIã§SEOåˆ†æä¸­...")
        analysis = self.analyze_with_claude(page_info)
        
        result = {
            'page_info': page_info,
            'seo_analysis': analysis
        }
        
        saved_path = self.save_analysis_data(url, result)
        print(f"ğŸ’¾ åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {saved_path}")
        
        return result

def main():
    analyzer = SEOAnalyzer()
    
    print("ğŸ” SEOåˆ†æãƒ„ãƒ¼ãƒ«")
    print("="*50)
    
    while True:
        url = input("\nåˆ†æã™ã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ã¾ãŸã¯qã§çµ‚äº†): ").strip()
        
        if url.lower() == 'q':
            print("ãƒ„ãƒ¼ãƒ«ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
            
        if not url:
            print("URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            continue
            
        if not (url.startswith('http://') or url.startswith('https://')):
            print("æœ‰åŠ¹ãªURLï¼ˆhttp://ã¾ãŸã¯https://ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            continue
    
        try:
            result = analyzer.analyze_url(url)
            
            if result is None:
                continue
            
            print("\n" + "="*80)
            print("ğŸ¯ SEOåˆ†æçµæœ")
            print("="*80)
            print(result['seo_analysis'])
            print("\n" + "="*80)
            
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f'seo_analysis_{timestamp}.txt'
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n")
                f.write(f"åˆ†ææ—¥æ™‚: {timestamp}\n")
                f.write("="*80 + "\n")
                f.write(result['seo_analysis'])
                f.write("\n" + "="*80 + "\n")
                f.write(f"è©³ç´°ãƒ‡ãƒ¼ã‚¿: {json.dumps(result['page_info'], ensure_ascii=False, indent=2)}")
            
            print(f"ğŸ“ åˆ†æçµæœã‚’{filename}ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        print("\n" + "-"*50)

if __name__ == "__main__":
    main()